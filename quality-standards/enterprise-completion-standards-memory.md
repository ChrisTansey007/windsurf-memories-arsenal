---
id: mem.enterprise-completion-standards
type: memory
title: Enterprise Completion Standards
tags: [quality, enterprise, completion, standards, audit-trail, compliance]
summary: Team-specific DONE definitions, acceptance gates, and quality standards for enterprise-grade work. Complements the Complete Problem-Solving rule.
version: 1
last_updated: 2025-10-31
---

# Enterprise Completion Standards Memory

**What Cascade remembers:** Your organization's quality standards, acceptance criteria, and what "DONE" means for different types of work.

**Works with:** [Complete Problem-Solving Rule](https://github.com/ChrisTansey007/ai-rules-arsenal/blob/main/windsurf/by-domain/complete-problem-solving.md)

---

## ðŸŽ¯ Purpose

This memory defines **team-specific completion standards** that go beyond the universal Complete Problem-Solving rule. It captures:
- Your organization's DONE definitions
- Team-specific acceptance gates
- Quality thresholds and SLAs
- Compliance requirements
- Evidence standards

---

## ðŸ“‹ Our Team's DONE Definitions

### Software/DevOps Work

**Standard DONE criteria:**
- âœ… **Health checks pass:** Primary endpoint returns 200 for â‰¥30 minutes
- âœ… **Zero errors:** Logs show no errors above INFO level (last 2 minutes)
- âœ… **Tests pass:** Build + unit tests + integration tests + type/lint = 0 failures
- âœ… **Code coverage:** â‰¥80% coverage for new code, â‰¥70% overall
- âœ… **Performance:** p95 latency < 200ms, p99 < 500ms
- âœ… **Security:** No high/critical vulnerabilities in scan
- âœ… **Documentation:** README updated, API docs current, changelog entry added
- âœ… **Review:** At least 1 peer approval on PR
- âœ… **Deployment:** Successful deploy to staging, smoke tests pass

**Critical/Production work adds:**
- âœ… **Runbook:** Incident response documented
- âœ… **Monitoring:** Alerts configured for key metrics
- âœ… **Rollback plan:** Documented and tested
- âœ… **Stakeholder approval:** Product/Engineering manager sign-off

---

### Data/Analytics Work

**Standard DONE criteria:**
- âœ… **Objective met:** Target metric achieved (state exact threshold)
- âœ… **Statistical validity:** p < 0.05 or confidence interval excludes 0
- âœ… **Sample size:** Power analysis confirms adequate sample (â‰¥80% power)
- âœ… **Reproducibility:** Notebook + dataset hash provided, re-run confirms results
- âœ… **Peer review:** Data scientist peer validation
- âœ… **Visualization:** Clear plots/dashboards for stakeholders
- âœ… **Documentation:** Analysis doc with methodology, assumptions, limitations
- âœ… **Data quality:** Missing data < 5%, outliers investigated

**Critical decisions add:**
- âœ… **Independent validation:** Re-run with holdout sample or different time period
- âœ… **Sensitivity analysis:** Key assumptions tested
- âœ… **Stakeholder review:** Business owner approval
- âœ… **Monitoring plan:** Metrics tracked post-launch

---

### Research/Writing/Documentation

**Standard DONE criteria:**
- âœ… **Sources:** â‰¥3 primary/authoritative sources for key claims
- âœ… **Citations:** All claims cited with links/references
- âœ… **Counterevidence:** Major counterarguments addressed
- âœ… **Clarity:** Executive summary + key takeaways + limitations
- âœ… **Accuracy:** Technical details verified by SME
- âœ… **Completeness:** All required sections present
- âœ… **Readability:** Grammarly/spell-check clean, reading level appropriate

**External-facing docs add:**
- âœ… **Legal review:** Terms reviewed by legal (if applicable)
- âœ… **Brand compliance:** Follows brand guidelines
- âœ… **Accessibility:** WCAG 2.1 AA compliant (for web docs)
- âœ… **Stakeholder approval:** Marketing/Product sign-off

---

### Product/UX Work

**Standard DONE criteria:**
- âœ… **Prototype:** Interactive artifact available (link or build)
- âœ… **Core flow:** Primary user journey implemented
- âœ… **Usability:** Task success rate â‰¥90% on Nâ‰¥5 users
- âœ… **Accessibility:** WCAG 2.1 AA compliant, keyboard navigation works
- âœ… **Responsive:** Works on mobile, tablet, desktop
- âœ… **Performance:** Page load < 3s, interaction < 100ms
- âœ… **Documentation:** User flow diagrams, design specs, known limitations

**Launch-ready adds:**
- âœ… **Analytics:** Event tracking implemented and tested
- âœ… **A/B test:** Experiment configured (if applicable)
- âœ… **Stakeholder approval:** Product manager + Design lead sign-off
- âœ… **Support docs:** Help articles or tooltips ready

---

### Security/Compliance Work

**Standard DONE criteria:**
- âœ… **Vulnerability scan:** No high/critical issues (CVSS â‰¥7.0)
- âœ… **Dependency audit:** All dependencies up-to-date, no known CVEs
- âœ… **Code review:** Security-focused review completed
- âœ… **Secrets:** No hardcoded secrets, all in vault/env vars
- âœ… **Authentication:** Proper auth/authz on all endpoints
- âœ… **Input validation:** All user inputs sanitized
- âœ… **Logging:** Security events logged (auth failures, permission denials)

**Compliance-critical adds:**
- âœ… **Audit trail:** All changes logged with timestamps and actors
- âœ… **Data privacy:** GDPR/CCPA requirements met (if applicable)
- âœ… **Penetration test:** External security audit passed
- âœ… **Compliance sign-off:** Security officer approval

---

## ðŸŽ¯ Quality Thresholds

### Code Quality
- **Test coverage:** â‰¥80% for new code, â‰¥70% overall
- **Cyclomatic complexity:** â‰¤10 per function
- **Code duplication:** <3% duplicate code
- **Linting:** 0 errors, <5 warnings
- **Type safety:** 100% type coverage (TypeScript/Python)

### Performance
- **API latency:** p95 < 200ms, p99 < 500ms
- **Page load:** First Contentful Paint < 1.5s, Time to Interactive < 3s
- **Database queries:** <100ms for simple queries, <500ms for complex
- **Memory usage:** <500MB per service instance

### Reliability
- **Uptime:** 99.9% SLA (43 minutes downtime/month max)
- **Error rate:** <0.1% of requests
- **Recovery time:** <15 minutes for P0 incidents
- **Data loss:** Zero tolerance

---

## ðŸ“Š Evidence Standards

### What to Include in Evidence Packs

**For Production Deployments:**
1. Health check screenshot/link (30+ minutes uptime)
2. Log excerpt (last 2 minutes, no errors)
3. Test results summary (all passing)
4. Performance metrics (latency, error rate)
5. PR link with approvals
6. Deployment timestamp and version

**For Data Analysis:**
1. Statistical summary (p-values, confidence intervals, effect sizes)
2. Plots/visualizations (with clear labels)
3. Notebook link + dataset hash
4. Peer review notes
5. Dashboard link (if applicable)

**For Security Work:**
1. Vulnerability scan report
2. Dependency audit results
3. Code review notes (security-focused)
4. Penetration test results (if applicable)
5. Compliance checklist (completed)

**For Documentation:**
1. Published doc link
2. Review/approval trail
3. Grammarly/spell-check results
4. Accessibility audit (if web doc)
5. Stakeholder sign-offs

---

## ðŸ”„ When to Use Complete-Mode

### Always Use (Mandatory)

- âœ… Production incidents (P0/P1)
- âœ… Security vulnerabilities
- âœ… Compliance-related changes
- âœ… Data migrations
- âœ… Infrastructure changes
- âœ… Public-facing launches
- âœ… Revenue-impacting features

### Recommended Use

- âœ… Critical features (high user impact)
- âœ… Complex bug fixes (multiple root causes)
- âœ… Performance optimizations
- âœ… Database schema changes
- âœ… API contract changes
- âœ… Third-party integrations

### Optional Use

- âš ï¸ Internal tools (lower stakes)
- âš ï¸ Experimental features (behind feature flags)
- âš ï¸ Documentation updates (non-critical)
- âš ï¸ Refactoring (no behavior change)

### Skip (Use Normal Workflow)

- âŒ Typo fixes
- âŒ Comment updates
- âŒ Dependency version bumps (patch versions)
- âŒ Simple UI tweaks

---

## ðŸŽ“ Team-Specific DONE Overlays

### Our API Standard

```markdown
[DONE overlay â€” Our Team's API Standard]
- OpenAPI spec: Updated and validated (swagger-cli validate passes)
- Tests: Integration tests â‰¥90% coverage, all passing
- Performance: p95 < 200ms, p99 < 500ms (load test with 1000 RPS)
- Security: Scan clean (no high/critical), auth on all endpoints
- Documentation: API docs published, changelog updated, migration guide (if breaking)
- Monitoring: Alerts configured (error rate >1%, latency >500ms)
- Stakeholder approval: Product manager + Tech lead sign-off
```

### Our Frontend Standard

```markdown
[DONE overlay â€” Our Team's Frontend Standard]
- Build: Production build succeeds, bundle size <500KB
- Tests: Unit tests â‰¥80% coverage, E2E tests for critical flows
- Performance: Lighthouse score â‰¥90 (Performance, Accessibility, Best Practices)
- Accessibility: WCAG 2.1 AA compliant, keyboard navigation works
- Responsive: Works on mobile (375px), tablet (768px), desktop (1440px)
- Browser support: Chrome, Firefox, Safari, Edge (latest 2 versions)
- Documentation: Component docs in Storybook, design specs linked
- Review: Design lead approval + peer code review
```

### Our Data Pipeline Standard

```markdown
[DONE overlay â€” Our Team's Data Pipeline Standard]
- Data quality: <1% missing data, outliers investigated
- Performance: Pipeline completes in <30 minutes (for daily batch)
- Monitoring: Data quality alerts configured, pipeline failure alerts
- Reproducibility: SQL/code in version control, data lineage documented
- Testing: Unit tests for transformations, integration test with sample data
- Documentation: Pipeline diagram, data dictionary, runbook for failures
- Validation: Output validated against known-good dataset
- Stakeholder approval: Data engineering lead sign-off
```

---

## ðŸš¨ Incident Response Standards

### P0 (Critical) Incidents

**DONE criteria:**
- âœ… Service restored (health checks passing â‰¥30 minutes)
- âœ… Root cause identified and documented
- âœ… Permanent fix deployed (not just workaround)
- âœ… Monitoring/alerting improved (prevent recurrence)
- âœ… Runbook updated
- âœ… Post-mortem completed (within 48 hours)
- âœ… Action items assigned and tracked

**Evidence pack:**
- Incident timeline (start, detection, mitigation, resolution)
- Root cause analysis
- Fix PR with before/after metrics
- Updated runbook
- Post-mortem doc link

### P1 (High) Incidents

**DONE criteria:**
- âœ… Issue resolved (verified by reporter)
- âœ… Root cause documented
- âœ… Fix deployed and verified
- âœ… Monitoring improved (if applicable)
- âœ… Brief incident summary

**Evidence pack:**
- Issue description + resolution
- Fix PR
- Verification results

---

## ðŸ“ˆ Success Metrics

### Track These Metrics

**Quality:**
- % of work completed on first attempt (target: â‰¥95%)
- Rework rate (target: <5%)
- Defect escape rate (target: <1%)

**Speed:**
- Time to DONE (track by work type)
- Time saved by avoiding rework
- Incident MTTR (Mean Time To Resolution)

**Compliance:**
- % of critical work with complete audit trails (target: 100%)
- % of security scans with 0 high/critical (target: â‰¥95%)
- % of compliance requirements met (target: 100%)

---

## ðŸ”— Related Arsenal Items

### Rules
- [Complete Problem-Solving](https://github.com/ChrisTansey007/ai-rules-arsenal/blob/main/windsurf/by-domain/complete-problem-solving.md) â­ Core rule
- [Prompt Quality Standards](https://github.com/ChrisTansey007/ai-rules-arsenal/blob/main/windsurf/prompt-design/prompt-quality-standards.md)
- [Next.js App Router](https://github.com/ChrisTansey007/ai-rules-arsenal/blob/main/windsurf/by-framework/nextjs-app-router.md)
- [FastAPI Python](https://github.com/ChrisTansey007/ai-rules-arsenal/blob/main/windsurf/by-framework/fastapi-python.md)

### Workflows
- [Run Tests and Fix](https://github.com/ChrisTansey007/ai-workflows-arsenal) - Automated testing
- [Security Scan](https://github.com/ChrisTansey007/ai-workflows-arsenal) - Security validation
- [Code Review Assistant](https://github.com/ChrisTansey007/ai-workflows-arsenal) - Pre-commit review

### Prompts
- [Self-Score Output](https://github.com/ChrisTansey007/prompt-arsenal/blob/main/quality-assurance/self-score-output.md) - Quality validation
- [Prompt Forensics Analyzer](https://github.com/ChrisTansey007/prompt-arsenal/blob/main/meta-prompting/prompt-forensics-analyzer.md) - Post-completion analysis

### Examples
- [Enterprise Quality Setup](https://github.com/ChrisTansey007/arsenal-integration-hub/tree/main/examples/enterprise-quality) - Complete integration guide

---

## ðŸ’¡ Customization Guide

### How to Customize This Memory

1. **Update quality thresholds** to match your team's standards
2. **Add team-specific DONE overlays** for your common work types
3. **Define your SLAs** and performance targets
4. **Document your compliance requirements** (GDPR, SOC2, HIPAA, etc.)
5. **Add your monitoring/alerting standards**
6. **Include your review/approval processes**

### Example: Adding a New Overlay

```markdown
### Our Mobile App Standard

[DONE overlay â€” Our Team's Mobile App Standard]
- Build: iOS + Android builds succeed, no warnings
- Tests: Unit tests â‰¥80%, UI tests for critical flows
- Performance: App launch <2s, screen transitions <300ms
- Size: APK <50MB, IPA <100MB
- Compatibility: iOS 14+, Android 10+
- Accessibility: VoiceOver/TalkBack support, dynamic type
- Store: Screenshots updated, release notes written
- Review: Mobile lead approval + QA sign-off
```

---

## ðŸ“ Usage Examples

### Example 1: Using with Complete-Mode

```
@complete-mode
Task = Fix payment processing bug in production
Context = Stripe integration; affecting 10% of transactions; $50k/day revenue impact

[DONE overlay â€” Our Team's API Standard]
scope:payment-flow depth:deep risk_tolerance:low strict:on
```

Cascade will use both:
- **Complete-mode rule** (universal framework)
- **This memory** (team-specific API standard)

### Example 2: Quick Reference During Work

When working on a feature, check this memory:
- "What's our DONE criteria for frontend work?"
- "What evidence do I need for the PR?"
- "Do I need complete-mode for this change?"

### Example 3: Onboarding New Team Members

New developers read this memory to understand:
- What "DONE" means on this team
- Quality standards and thresholds
- When to use rigorous vs. normal workflow
- What evidence is expected

---

## ðŸŽ¯ Quick Reference

### When in Doubt

**Ask yourself:**
1. Is this production-critical? â†’ Use complete-mode
2. Does this affect users/revenue? â†’ Use complete-mode
3. Is this compliance-related? â†’ Use complete-mode
4. Could this cause an incident? â†’ Use complete-mode
5. Is this a simple, low-risk change? â†’ Normal workflow is fine

**Remember:**
- Complete-mode is for **critical work**, not everything
- Use `depth:shallow` for lighter rigor when needed
- Evidence packs make reviews faster, not slower
- Audit trails protect you and the team

---

**Last Updated:** 2025-10-31  
**Maintained By:** Engineering Team  
**Review Frequency:** Quarterly

---

**This memory ensures Cascade knows your team's professional standards!** ðŸŽ¯
